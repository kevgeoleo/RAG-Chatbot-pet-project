import os
from sentence_transformers import SentenceTransformer
import numpy as np
import json

cwd = os.getcwd()
input_dataset = os.path.join(cwd,"data","dataset.txt")

# Step 1: Load and split the dataset - into meaningful chunks 
def load_and_split_dataset(file_path, min_chunk_length=50):
    with open(file_path, "r", encoding="utf-8") as f:
        raw_text = f.read()

    # Split by paragraph 
    paragraphs = raw_text.split("\n\n")
    chunks = [p.strip().replace("\n", " ") for p in paragraphs if len(p.strip()) > min_chunk_length]
    return chunks

# Step 2:  Generate embeddings for each chunk - transforms chunks generated by Step 1 into vector
# chunks with similar meaning will be similar in vector space  
#all-MiniLM-L6-v2 is a pretrained model 
def generate_embeddings(chunks, model_name="all-MiniLM-L6-v2"): 
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks, convert_to_numpy=True)
    return embeddings

# Step 3: Save chunks and embeddings for later use
# save the vector space of chunks so that you dont have to recalculate it 
# When user ask a question, you encode that as well and compare vectors to find which chunk matches more closely
def save_chunks_and_embeddings(chunks, embeddings, out_dir="processed"):
    os.makedirs(out_dir, exist_ok=True)

    #chunks from Step 1 saved as json 
    with open(os.path.join(out_dir, "chunks.json"), "w", encoding="utf-8") as f:
        json.dump(chunks, f, indent=2, ensure_ascii=False)

    #embeddings from Step 2 saved as numpy binary format
    np.save(os.path.join(out_dir, "embeddings.npy"), embeddings)

# Main preprocessing function
def preprocess(file_path):
    print("Step 1: Loading and splitting dataset")
    chunks = load_and_split_dataset(file_path)
    print(f"Loaded {len(chunks)} chunks.")

    print("Step 2: Generating embeddings")
    embeddings = generate_embeddings(chunks)
    print(f"Generated {embeddings.shape[0]} embeddings.")

    print("Step 3: Saving to disk")
    save_chunks_and_embeddings(chunks, embeddings)
    print("Success")

if __name__ == "__main__":
    preprocess(input_dataset)
